{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6bff9d1",
   "metadata": {},
   "source": [
    "# News Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15761b0",
   "metadata": {},
   "source": [
    "### Required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "004e5ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "from core.ukraine_utilities import NewsPreprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea181da9",
   "metadata": {},
   "source": [
    "### Defining paths and directory creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43c51184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/train.json\n",
      "data/test_cleaned.csv\n",
      "data/train_cleaned.csv\n",
      "data/test.json\n",
      "data/twitter_train.csv\n"
     ]
    }
   ],
   "source": [
    "# define Path object for data directory\n",
    "root_dir = Path('./')\n",
    "data_dir = root_dir / 'data'\n",
    "models_dir = root_dir / 'models'\n",
    "plots_dir = root_dir / 'plots'\n",
    "\n",
    "# print data files\n",
    "for data_file in data_dir.glob('*'):\n",
    "    print(data_file)\n",
    "    \n",
    "# create directory for plots and models\n",
    "plots_dir.mkdir(exist_ok=True)\n",
    "models_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9351c30",
   "metadata": {},
   "source": [
    "### Loading in datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e242e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2056, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Как сообщает пресс-служба акимата Алматы, для ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Казахстанские авиакомпании перевозят 250 тысяч...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>На состоявшемся под председательством Касым-Жо...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>В ОАЭ состоялись переговоры между казахстанско...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 вагонов грузового поезда сошли с путей в Во...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  id\n",
       "0  Как сообщает пресс-служба акимата Алматы, для ...   0\n",
       "1  Казахстанские авиакомпании перевозят 250 тысяч...   1\n",
       "2  На состоявшемся под председательством Касым-Жо...   2\n",
       "3  В ОАЭ состоялись переговоры между казахстанско...   3\n",
       "4  12 вагонов грузового поезда сошли с путей в Во...   4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_json(data_dir / 'train.json')\n",
    "test = pd.read_json(data_dir / 'test.json')\n",
    "\n",
    "print(test.shape)\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93233b32",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80390e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on NewsPreprocessor in module core.ukraine_utilities object:\n",
      "\n",
      "class NewsPreprocessor(builtins.object)\n",
      " |  NewsPreprocessor(X_train, y_train)\n",
      " |  \n",
      " |  all functions are applied at the sample level.\n",
      " |  for a pandas.DataFrame object do\n",
      " |  df.apply(lambda sample: NewsProcessor.method(sample))\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, X_train, y_train)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  get_common_enough_words(self, filter_val)\n",
      " |      get all words that are present more times than threshold value filter_val\n",
      " |  \n",
      " |  get_freqwords(self, num_words)\n",
      " |      get the num_words most frequent words in the object instance of vocab\n",
      " |  \n",
      " |  get_wordcount(self)\n",
      " |      populate the object instance of vocab (collections.Counter) with words in X_train\n",
      " |  \n",
      " |  label_encoder(self)\n",
      " |  \n",
      " |  remove_freqwords(self, doc)\n",
      " |      remove the frequent words\n",
      " |  \n",
      " |  remove_numbers(self, doc)\n",
      " |      remove numbers from the text\n",
      " |  \n",
      " |  remove_rarewords(self, doc)\n",
      " |      remove the rare words\n",
      " |  \n",
      " |  remove_stopwords(self, doc)\n",
      " |      remove stopwords from the text\n",
      " |  \n",
      " |  stemmer(self, doc)\n",
      " |  \n",
      " |  strip_html_tags(self, doc)\n",
      " |      remove HTML tags from the text\n",
      " |  \n",
      " |  strip_punctuation(self, doc)\n",
      " |  \n",
      " |  strip_special_chars(self, doc)\n",
      " |      remove special characters from the text\n",
      " |  \n",
      " |  to_lowercase(self, doc)\n",
      " |      convert all text to lowercase and remove newline characters\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test = train['text'], train['sentiment'], test['text']\n",
    "\n",
    "processor = NewsPreprocessor(X_train, y_train)\n",
    "\n",
    "help(processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4bd2ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to_lowercase executed\n",
      "strip_punctuation executed\n",
      "remove_stopwords executed\n",
      "strip_special_chars executed\n",
      "remove_numbers executed\n",
      "stemmer executed\n",
      "label encoder executed\n"
     ]
    }
   ],
   "source": [
    "# preprocess X_train and store in train_cleaned.csv\n",
    "X_train = X_train.apply(lambda doc: processor.to_lowercase(doc))\n",
    "print(\"to_lowercase executed\")\n",
    "X_train = X_train.apply(lambda doc: processor.strip_punctuation(doc))\n",
    "print(\"strip_punctuation executed\")\n",
    "X_train = X_train.apply(lambda doc: processor.remove_stopwords(doc))\n",
    "print(\"remove_stopwords executed\")\n",
    "X_train = X_train.apply(lambda doc: processor.strip_special_chars(doc))\n",
    "print(\"strip_special_chars executed\")\n",
    "X_train = X_train.apply(lambda doc: processor.remove_numbers(doc))\n",
    "print(\"remove_numbers executed\")\n",
    "# processor.get_wordcount()\n",
    "# processor.get_freqwords(100)\n",
    "# X_train = X_train.apply(lambda doc: processor.remove_freqwords(doc))\n",
    "# print(\"remove_freqwords executed\")\n",
    "# processor.get_common_enough_words(3)\n",
    "# X_train = X_train.apply(lambda doc: processor.remove_rarewords(doc))\n",
    "# print(\"remove_rarewords executed\")\n",
    "X_train = X_train.apply(lambda doc: processor.stemmer(doc))\n",
    "print(\"stemmer executed\")\n",
    "y_train = processor.label_encoder()\n",
    "print(\"label encoder executed\")\n",
    "\n",
    "train_cleaned = pd.DataFrame({'text': X_train,\n",
    "                              'sentiment': y_train})\n",
    "\n",
    "train_cleaned.to_csv(data_dir / 'train_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d55f9018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to_lowercase executed\n",
      "strip_punctuation executed\n",
      "remove_stopwords executed\n",
      "strip_special_chars executed\n",
      "remove_numbers executed\n",
      "stemmer executed\n"
     ]
    }
   ],
   "source": [
    "# preprocess X_test and store in test_cleaned.csv\n",
    "X_test = X_test.apply(lambda doc: processor.to_lowercase(doc))\n",
    "print(\"to_lowercase executed\")\n",
    "X_test = X_test.apply(lambda doc: processor.strip_punctuation(doc))\n",
    "print(\"strip_punctuation executed\")\n",
    "X_test = X_test.apply(lambda doc: processor.remove_stopwords(doc))\n",
    "print(\"remove_stopwords executed\")\n",
    "X_test = X_test.apply(lambda doc: processor.strip_special_chars(doc))\n",
    "print(\"strip_special_chars executed\")\n",
    "X_test = X_test.apply(lambda doc: processor.remove_numbers(doc))\n",
    "print(\"remove_numbers executed\")\n",
    "# processor.get_wordcount()\n",
    "# processor.get_freqwords(10)\n",
    "# X_test = X_test.apply(lambda doc: processor.remove_freqwords(doc))\n",
    "# print(\"remove_freqwords executed\")\n",
    "# processor.get_common_enough_words(3)\n",
    "# X_test = X_test.apply(lambda doc: processor.remove_rarewords(doc))\n",
    "# print(\"remove_rarewords executed\")\n",
    "X_test = X_test.apply(lambda doc: processor.stemmer(doc))\n",
    "print(\"stemmer executed\")\n",
    "\n",
    "test_cleaned = pd.DataFrame({'text': X_test})\n",
    "\n",
    "test_cleaned.to_csv(data_dir / 'test_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9338d0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'досудебн расследован факт покупк енпф пакет облигац то бузгул аурум начат инициатив национальн банк рк сообщ директор департамент защит прав потребител финансов услуг нацбанк казахста александр терент основан досудебн расследован стал обращен национальн банк письм  ноябр  год обращен национальн банк правоохранительн орга нам эт сделк показа сомнительн недостаточн корректн поэт нацбанк  ноябр  год обрат правоохранительн орга эт мог озвуч сегодн идет следств провод проверк   сказа терент  декабр нацбанк заяв знают стал основан проверк енпф  декабр факт проведен проверк а един накопительн пенсион фонд подтверд прессслужб национальн банк сообщ проверк провод операц совершен а енпф отношен инвестирован собствен актив такж финрегулятор сообща сделк енпф сумм пят млрд завед уголовн дел нацбанк заверя все происходя затрагива пенсион накоплен казахстанц нашл ошибк текст выдел мыш нажм ctrlenter '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cleaned.text[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
